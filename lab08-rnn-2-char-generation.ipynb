{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This Lab is made with inspiration & code snippets of:\n",
    "- https://github.com/yxtay/char-rnn-text-generation\n",
    "- https://d2l.ai/chapter_recurrent-modern/lstm.html\n",
    "- https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as F\n",
    "import mxnet.gluon as gluon\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import autograd\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.text_helpers import load_corpus, Vocab, batch_generator, encode_text, decode_text, generate_seed, sample_from_probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(gluon.Block):\n",
    "    \"\"\"\n",
    "    build character embeddings LSTM text generation model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_size=32,\n",
    "                 rnn_size=128, num_layers=2, drop_rate=0.0, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "\n",
    "        self.args = {\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"embedding_size\": embedding_size,\n",
    "            \"rnn_size\": rnn_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"drop_rate\": drop_rate}\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.encoder = nn.Embedding(vocab_size, embedding_size)\n",
    "            self.dropout = nn.Dropout(drop_rate)\n",
    "            self.rnn = rnn.LSTM(rnn_size, num_layers, dropout = drop_rate,\n",
    "                                input_size = embedding_size)\n",
    "            self.decoder = nn.Dense(vocab_size, in_units = rnn_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # input shape: [seq_len, batch_size]\n",
    "        seq_len, batch_size = inputs.shape\n",
    "        embed_seq = self.dropout(self.encoder(inputs))\n",
    "        # shape: [seq_len, batch_size, embedding_size]\n",
    "        rnn_out, state = self.rnn(embed_seq, state)\n",
    "        # rnn_out shape: [seq_len, batch_size, rnn_size]\n",
    "        # hidden shape: [2, num_layers, batch_size, rnn_size]\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        # shape: [seq_len, batch_size, rnn_size]\n",
    "        logits = (self.decoder(rnn_out.reshape((-1, rnn_out.shape[2])))\n",
    "                  .reshape((seq_len, batch_size, -1)))\n",
    "        # output shape: [seq_len, batch_size, vocab_size]\n",
    "        return logits, state\n",
    "\n",
    "    def begin_state(self, batch_size=1):\n",
    "        \"\"\"\n",
    "        initialises rnn states.\n",
    "        \"\"\"\n",
    "        return self.rnn.begin_state(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(corpus, vocab, embedding_size=32,\n",
    "          rnn_size=128, num_layers=2, drop_rate=0.0,\n",
    "          batch_size = 64, seq_len = 64, num_epochs = 64):\n",
    "    \"\"\"\n",
    "    trains model specfied in args.\n",
    "    main method for train subcommand.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"corpus length: %s, vocabulary size: %s\" %(len(corpus), len(vocab)))\n",
    "\n",
    "    VOCAB_SIZE = len(vocab)\n",
    "\n",
    "    model = Model(vocab_size=VOCAB_SIZE,\n",
    "                  embedding_size=embedding_size,\n",
    "                  rnn_size=rnn_size,\n",
    "                  num_layers=num_layers,\n",
    "                  drop_rate=drop_rate)\n",
    "    model.initialize(mx.init.Xavier())\n",
    "    model.hybridize()\n",
    "\n",
    "    # loss function\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss(batch_axis=1)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = mx.optimizer.Adam(learning_rate=0.001, clip_gradient=5.0)\n",
    "\n",
    "    # trainer\n",
    "    trainer = gluon.Trainer(model.collect_params(), optimizer)\n",
    "\n",
    "    # training start\n",
    "    num_batches = (len(corpus) - 1) // (batch_size * seq_len)\n",
    "    data_iter = batch_generator(encode_text(corpus, char2id=vocab), batch_size = batch_size, seq_len=seq_len, vocab = vocab)\n",
    "    state = model.begin_state(batch_size)\n",
    "\n",
    "    print(\"start of training.\")\n",
    "    time_train = time.time()\n",
    "    for i in range(num_epochs):\n",
    "        epoch_losses = mx.nd.empty(num_batches)\n",
    "        time_epoch = time.time()\n",
    "        # training epoch\n",
    "        for j in tqdm(range(num_batches), desc=\"epoch {}/{}\".format(i + 1, num_epochs), position=0, leave=True):\n",
    "            # prepare inputs\n",
    "            x, y = next(data_iter)\n",
    "            x = mx.nd.array(x.T)\n",
    "            y = mx.nd.array(y.T)\n",
    "            # reset state variables to remove their history\n",
    "            state = [arr.detach() for arr in state]\n",
    "\n",
    "            with autograd.record():\n",
    "                logits, state = model(x, state)\n",
    "                # calculate loss\n",
    "                L = loss(logits, y)\n",
    "                L = F.mean(L)\n",
    "                epoch_losses[j] = L.asscalar()\n",
    "                # calculate gradient\n",
    "                L.backward()\n",
    "            # apply gradient update\n",
    "            trainer.step(1)\n",
    "\n",
    "        # logs\n",
    "        duration_epoch = time.time() - time_epoch\n",
    "        print(\"epoch: %s, duration: %ds, loss: %.6g.\"\n",
    "              %(i + 1, duration_epoch, F.mean(epoch_losses).asscalar()))\n",
    "\n",
    "        # generate text\n",
    "        seed = generate_seed(corpus)\n",
    "        generate_text(model, seed, vocab=vocab)\n",
    "\n",
    "    # training end\n",
    "    duration_train = time.time() - time_train\n",
    "    print(\"end of training, duration: %ds.\" %duration_train)\n",
    "    # generate text\n",
    "    seed = generate_seed(corpus)\n",
    "    generate_text(model, seed, 1024, 3, vocab=vocab)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_text(model, seed, length=512, top_n=10, vocab=Vocab):\n",
    "    \"\"\"\n",
    "    generates text of specified length from trained model\n",
    "    with given seed character sequence.\n",
    "    \"\"\"\n",
    "    print(\"generating %s characters from top %s choices.\"%(length, top_n))\n",
    "    print('generating with seed: \"%s\".' % (''.join(seed))) # ['a', 'b', 'c'] -> \"abc\"\n",
    "\n",
    "    generated = seed\n",
    "    encoded = mx.nd.array(encode_text(seed, char2id=vocab))\n",
    "    seq_len = encoded.shape[0]\n",
    "\n",
    "    x = F.expand_dims(encoded[:seq_len-1], 1)\n",
    "    # input shape: [seq_len, 1]\n",
    "    state = model.begin_state()\n",
    "    # get rnn state due to seed sequence\n",
    "    _, state = model(x, state)\n",
    "\n",
    "    next_index = encoded[seq_len-1].asscalar()\n",
    "    for i in range(length):\n",
    "        x = mx.nd.array([[next_index]])\n",
    "        # input shape: [1, 1]\n",
    "        logit, state = model(x, state)\n",
    "        # output shape: [1, vocab_size]\n",
    "        probs = F.softmax(logit)\n",
    "        next_index = sample_from_probs(probs.asnumpy().squeeze(), top_n)\n",
    "        # append to sequence\n",
    "        generated += vocab.to_tokens(next_index) #ID2CHAR[next_index]\n",
    "\n",
    "    print(\"generated text: \\n%s\\n\" %(''.join(generated)))\n",
    "    return generated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read text\n",
    "# corpus, vocab = load_corpus('data/time-machine.txt')\n",
    "corpus, vocab = load_corpus('data/tinyshakespeare.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = train(corpus, vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model, seed, length=512, top_n=10, vocab=Vocab\n",
    "generate_text(model, \"to\", length=128, top_n=3, vocab=vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}